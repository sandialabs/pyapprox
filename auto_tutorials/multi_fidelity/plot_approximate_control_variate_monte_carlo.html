<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Approximate Control Variate Monte Carlo &mdash; PyApprox 1.0.0 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/plot_directive.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script>window.MathJax = {"tex": {"macros": {"V": ["{\\boldsymbol{#1}}", 1], "mean": ["{\\mathbb{E}\\left[#1\\right]}", 1], "var": ["{\\mathbb{V}\\left[#1\\right]}", 1], "rv": "{z}", "reals": "\\mathbb{R}", "pdf": "\\rho", "rvdom": "\\Gamma", "coloneqq": "\\colon=", "norm": ["{\\lVert #1 \\rVert}", 1], "argmax": ["\\operatorname{argmax}"], "argmin": ["\\operatorname{argmin}"], "covar": ["\\mathbb{C}\\text{ov}\\left[#1,#2\\right]", 2], "corr": ["\\mathbb{C}\\text{or}\\left[#1,#2\\right]", 2], "ai": "\\alpha", "bi": "\\beta", "dx": ["\\;\\mathrm{d}#1", 1]}}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Multi-level Monte Carlo" href="plot_multi_level_monte_carlo.html" />
    <link rel="prev" title="Control Variate Monte Carlo" href="plot_control_variate_monte_carlo.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> PyApprox
            <img src="../../_static/pyapprox-logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intro.html">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../auto_examples/index.html">Software Tutorials</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Theoretical Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#model-analysis">Model Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#inference">Inference</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#multi-fidelity-methods">Multi-Fidelity Methods</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="plot_monte_carlo.html">Monte Carlo Quadrature</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_control_variate_monte_carlo.html">Control Variate Monte Carlo</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Approximate Control Variate Monte Carlo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_multi_level_monte_carlo.html">Multi-level Monte Carlo</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_multi_fidelity_monte_carlo.html">Multi-fidelity Monte Carlo</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_many_model_approximate_control_variate_monte_carlo.html">Generalized Approximate Control Variate Monte Carlo</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_gaussian_mfnets.html">MFNets: Multi-fidelity networks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#surrogates">Surrogates</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmarks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Reference Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user_reference_guide.html">User Reference Guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">PyApprox</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../index.html">Theoretical Tutorials</a> &raquo;</li>
      <li>Approximate Control Variate Monte Carlo</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/auto_tutorials/multi_fidelity/plot_approximate_control_variate_monte_carlo.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-auto-tutorials-multi-fidelity-plot-approximate-control-variate-monte-carlo-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="approximate-control-variate-monte-carlo">
<span id="sphx-glr-auto-tutorials-multi-fidelity-plot-approximate-control-variate-monte-carlo-py"></span><h1>Approximate Control Variate Monte Carlo<a class="headerlink" href="#approximate-control-variate-monte-carlo" title="Permalink to this headline"></a></h1>
<p>This tutorial builds upon <a class="reference internal" href="plot_control_variate_monte_carlo.html#sphx-glr-auto-tutorials-multi-fidelity-plot-control-variate-monte-carlo-py"><span class="std std-ref">Control Variate Monte Carlo</span></a> and describes how to implement and deploy <em>approximate</em> control variate Monte Carlo (ACVMC) sampling to compute expectations of model output from multiple low-fidelity models with unknown means.</p>
<p>CVMC is often not useful for practical analysis of numerical models because typically the mean of the lower fidelity model, i.e. <span class="math notranslate nohighlight">\(\mu_\V{\kappa}\)</span>, is unknown and the cost of the lower fidelity model is non trivial. These two issues can be overcome by using approximate control variate Monte Carlo.</p>
<p>Let the cost of the high fidelity model per sample be <span class="math notranslate nohighlight">\(C_\alpha\)</span> and let the cost of the low fidelity model be <span class="math notranslate nohighlight">\(C_\kappa\)</span>. Now lets use <span class="math notranslate nohighlight">\(N\)</span> samples to estimate <span class="math notranslate nohighlight">\(Q_{\V{\alpha},N}\)</span> and <span class="math notranslate nohighlight">\(Q_{\V{\kappa},N}\)</span> and these  <span class="math notranslate nohighlight">\(N\)</span> samples plus another <span class="math notranslate nohighlight">\((r-1)N\)</span> samples to estimate <span class="math notranslate nohighlight">\(\mu_{\V{\kappa}}\)</span> so that</p>
<div class="math notranslate nohighlight">
\[Q_{\V{\alpha},N,r}^{\text{ACV}}=Q_{\V{\alpha},N} + \eta \left( Q_{\V{\kappa},N} - \mu_{\V{\kappa},N,r} \right)\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\mu_{\V{\kappa},N,r}=\frac{1}{rN}\sum_{i=1}^{rN}Q_\V{\kappa}\]</div>
<p>With this sampling scheme we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}Q_{\V{\kappa},N} - \mu_{\V{\kappa},N,r}&amp;=\frac{1}{N}\sum_{i=1}^N f_\V{\kappa}^{(i)}-\frac{1}{rN}\sum_{i=1}^{rN}f_\V{\kappa}^{(i)}\\
&amp;=\frac{1}{N}\sum_{i=1}^N f_\V{\kappa}^{(i)}-\frac{1}{rN}\sum_{i=1}^{N}f_\V{\kappa}^{(i)}-\frac{1}{rN}\sum_{i=N}^{rN}f_\V{\kappa}^{(i)}\\
&amp;=\frac{r-1}{rN}\sum_{i=1}^N f_\V{\kappa}^{(i)}-\frac{1}{rN}\sum_{i=N}^{rN}f_\V{\kappa}^{(i)}\\\end{split}\]</div>
<p>where for ease of notation we write <span class="math notranslate nohighlight">\(r_\V{\kappa}N\)</span> and <span class="math notranslate nohighlight">\(\lfloor r_\V{\kappa}N\rfloor\)</span> interchangibly.
Using the above expression yields</p>
<div class="math notranslate nohighlight">
\[\begin{split}\var{\left( Q_{\V{\kappa},N} - \mu_{\V{\kappa},N,r}\right)}&amp;=\mean{\left(\frac{r-1}{rN}\sum_{i=1}^N f_\V{\kappa}^{(i)}-\frac{1}{rN}\sum_{i=N}^{rN}f_\V{\kappa}^{(i)}\right)^2}\\
&amp;=\frac{(r-1)^2}{r^2N^2}\sum_{i=1}^N \var{f_\V{\kappa}^{(i)}}+\frac{1}{r^2N^2}\sum_{i=N}^{rN}\var{f_\V{\kappa}^{(i)}}\\
&amp;=\frac{(r-1)^2}{r^2N^2}N\var{f_\V{\kappa}}+\frac{1}{r^2N^2}(r-1)N\var{f_\V{\kappa}}\\
%&amp;=\left(\frac{(r-1)^2}{r^2N}+\frac{(r-1)}{r^2N}\right)\var{f_\V{\kappa}}\\
&amp;=\frac{r-1}{r}\frac{\var{f_\V{\kappa}}}{N}\end{split}\]</div>
<p>where we have used the fact that since the samples used in the first and second term on the first line are not shared, the covariance between these terms is zero. Also we have</p>
<div class="math notranslate nohighlight">
\[\covar{Q_{\V{\alpha},N}}{\left( Q_{\V{\kappa},N} - \mu_{\V{\kappa},N,r}\right)}=\covar{\frac{1}{N}\sum_{i=1}^N f_\V{\alpha}^{(i)}}{\frac{r-1}{rN}\sum_{i=1}^N f_\V{\kappa}^{(i)}-\frac{1}{rN}\sum_{i=N}^{rN}f_\V{\kappa}^{(i)}}\]</div>
<p>The correlation between the estimators <span class="math notranslate nohighlight">\(\frac{1}{N}\sum_{i=1}^{N}Q_\V{\alpha}\)</span> and <span class="math notranslate nohighlight">\(\frac{1}{rN}\sum_{i=N}^{rN}Q_\V{\kappa}\)</span> is zero because the samples used in these estimators are different for each model. Thus</p>
<div class="math notranslate nohighlight">
\[\begin{split} \covar{Q_{\V{\alpha},N}}{\left( Q_{\V{\kappa},N} - \mu_{\V{\kappa},N,r}\right)} &amp;=\covar{\frac{1}{N}\sum_{i=1}^N f_\V{\alpha}^{(i)}}{\frac{r-1}{rN}\sum_{i=1}^N f_\V{\kappa}^{(i)}}\\
&amp;=\frac{r-1}{r}\frac{\covar{f_\V{\alpha}}{f_\V{\kappa}}}{N}\end{split}\]</div>
<p>Recalling the variance reduction of the CV estimator using the optimal <span class="math notranslate nohighlight">\(\eta\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\gamma &amp;= 1-\frac{\covar{Q_{\V{\alpha},N}}{\left( Q_{\V{\kappa},N} - \mu_{ \V{\kappa},N,r}\right)}^2}{\var{\left( Q_{\V{\kappa},N} - \mu_{\V{\kappa},N,r}\right)}\var{Q_{\V{\alpha},N}}}\\
&amp;=1-\frac{N^{-2}\frac{(r-1)^2}{r^2}\covar{f_\V{\alpha}}{f_\V{\kappa}}}{N^{-1}\frac{r-1}{r}\var{f_\V{\kappa}}N^{-1}\var{f_\V{\alpha}}}\\
&amp;=1-\frac{r-1}{r}\corr{f_\V{\alpha}}{f_\V{\kappa}}^2\end{split}\]</div>
<p>which is found when</p>
<div class="math notranslate nohighlight">
\[\begin{split} \eta&amp;=-\frac{\covar{Q_{\V{\alpha},N}}{\left( Q_{\V{\kappa},N} - \mu_{\V{\kappa},N,r}\right)}}{\var{\left( Q_{\V{\kappa},N} - \mu_{\V{\kappa},N,r}\right)}}\\
&amp;=-\frac{N^{-1}\frac{r-1}{r}\covar{f_\V{\alpha}}{f_\V{\kappa}}}{N^{-1}\frac{r-1}{r}\var{f_\V{\kappa}}}\\
&amp;=-\frac{\covar{f_\V{\alpha}}{f_\V{\kappa}}}{\var{f_\V{\kappa}}}\end{split}\]</div>
<p>Lets setup the problem and compute an ACV estimate of <span class="math notranslate nohighlight">\(\mean{f_0}\)</span></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">pyapprox.benchmarks</span> <span class="kn">import</span> <span class="n">setup_benchmark</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">shifts</span> <span class="o">=</span> <span class="p">[</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">]</span>
<span class="n">benchmark</span> <span class="o">=</span> <span class="n">setup_benchmark</span><span class="p">(</span>
    <span class="s2">&quot;tunable_model_ensemble&quot;</span><span class="p">,</span> <span class="n">theta1</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="mf">.95</span><span class="p">,</span> <span class="n">shifts</span><span class="o">=</span><span class="n">shifts</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">fun</span>
<span class="n">exact_integral_f0</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">means</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Before proceeding to estimate the mean using ACVMV we must first define how to generate samples to estimate <span class="math notranslate nohighlight">\(Q_{\V{\alpha},N}\)</span> and <span class="math notranslate nohighlight">\(\mu_{\V{\kappa},N,r}\)</span>. To do so clearly we must first introduce some additional notation. Let <span class="math notranslate nohighlight">\(\mathcal{Z}_0\)</span> be the set of samples used to evaluate the high-fidelity model and let <span class="math notranslate nohighlight">\(\mathcal{Z}_\alpha=\mathcal{Z}_{\alpha,1}\cup\mathcal{Z}_{\alpha,2}\)</span> be the samples used to evaluate the low fidelity model. Using this notation we can rewrite the ACV estimator as</p>
<div class="math notranslate nohighlight">
\[Q_{\V{\alpha},\mathcal{Z}}^{\text{ACV}}=Q_{\V{\alpha},\mathcal{Z}_0} + \eta \left( Q_{\V{\kappa},\mathcal{Z}_{\alpha,1}} - \mu_{\V{\kappa},\mathcal{Z}_{\alpha,2}} \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{Z}=\bigcup_{\alpha=0}^M Z_\alpha\)</span>. The nature of these samples can be changed to produce different ACV estimators. Here we choose  <span class="math notranslate nohighlight">\(\mathcal{Z}_{\alpha,1}\cap\mathcal{Z}_{\alpha,2}=\emptyset\)</span> and <span class="math notranslate nohighlight">\(\mathcal{Z}_{\alpha,1}=\mathcal{Z_0}\)</span>. That is we use the set a common set of samples to compute the covariance between all the models and a second independent set to estimate the lower fidelity mean. The sample partitioning for <span class="math notranslate nohighlight">\(M\)</span> models is  shown in the following Figure. We call this scheme the ACV IS sampling strategy where IS indicates that the second sample set <span class="math notranslate nohighlight">\(\mathcal{Z}_{\alpha,2}\)</span> assigned to each model are not shared.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><figure class="align-center" id="id1">
<span id="acv-is-sample-allocation"></span><a class="reference internal image-reference" href="../../_images/acv_is.png"><img alt="../../_images/acv_is.png" src="../../_images/acv_is.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-text">ACV IS sampling strategy</span><a class="headerlink" href="#id1" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</td>
</tr>
</tbody>
</table>
<p>The following code generates samples according to this strategy</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyapprox</span> <span class="kn">import</span> <span class="n">multifidelity</span>
<span class="n">model_costs</span> <span class="o">=</span> <span class="mf">10.</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="n">est</span> <span class="o">=</span> <span class="n">multifidelity</span><span class="o">.</span><span class="n">get_estimator</span><span class="p">(</span>
    <span class="s2">&quot;acvis&quot;</span><span class="p">,</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">model_covariance</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">model_costs</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span>
    <span class="n">benchmark</span><span class="o">.</span><span class="n">variable</span><span class="p">)</span>
<span class="n">est</span><span class="o">.</span><span class="n">nsamples_per_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
<span class="n">samples_per_model</span><span class="p">,</span> <span class="n">partition_indices_per_model</span> <span class="o">=</span> \
    <span class="n">est</span><span class="o">.</span><span class="n">generate_sample_allocations</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">partition_indices_per_model</span><span class="p">)</span>
<span class="n">samples_shared</span> <span class="o">=</span> <span class="n">samples_per_model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">samples_lf_only</span> <span class="o">=</span> <span class="n">samples_per_model</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="n">partition_indices_per_model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]
</pre></div>
</div>
<p>Now lets plot the samples assigned to each model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">samples_shared</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">samples_shared</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mathrm{Low\ and\  high\  fidelity\  models}$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">samples_lf_only</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">samples_lf_only</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="s1">&#39;ks&#39;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mathrm{Low\  fidelity\  model\ only}$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$z_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$z_2$&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_approximate_control_variate_monte_carlo_001.png" srcset="../../_images/sphx_glr_plot_approximate_control_variate_monte_carlo_001.png" alt="plot approximate control variate monte carlo" class = "sphx-glr-single-img"/><p>The high-fidelity model is only evaluated on the red dots. Now lets use these samples to estimate the mean of <span class="math notranslate nohighlight">\(f_0\)</span>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values_per_model</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples_per_model</span><span class="p">)):</span>
    <span class="n">values_per_model</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">models</span><span class="p">[</span><span class="n">ii</span><span class="p">](</span><span class="n">samples_per_model</span><span class="p">[</span><span class="n">ii</span><span class="p">]))</span>

<span class="n">acv_mean</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">estimate_from_values_per_model</span><span class="p">(</span>
    <span class="n">values_per_model</span><span class="p">,</span> <span class="n">partition_indices_per_model</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MC difference squared =&#39;</span><span class="p">,</span> <span class="p">(</span>
    <span class="n">values_per_model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">-</span><span class="n">exact_integral_f0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACVMC difference squared =&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">acv_mean</span><span class="o">-</span><span class="n">exact_integral_f0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>MC difference squared = 0.1663308327274871
ACVMC difference squared = 0.010697826122695035
</pre></div>
</div>
<p>Note here we have arbitrarily set the number of high fidelity samples <span class="math notranslate nohighlight">\(N\)</span> and the ratio <span class="math notranslate nohighlight">\(r\)</span>. In practice one should choose these in one of two ways: (i) for a fixed budget choose the free parameters to minimize the variance of the estimator; or (ii) choose the free parameters to achieve a desired MSE (variance) with the smallest computational cost. Note the cost of computing the two model ACV estimator is</p>
<div class="math notranslate nohighlight">
\[C_\mathrm{cv} = NC_\alpha + r_\V{\kappa}NC_\kappa\]</div>
<p>Now lets compute the variance reduction for different sample sizes</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyapprox</span> <span class="kn">import</span> <span class="n">interface</span>
<span class="n">model_ensemble</span> <span class="o">=</span> <span class="n">interface</span><span class="o">.</span><span class="n">ModelEnsemble</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">models</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">nhf_samples</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">ntrials</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">nsample_ratios</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
<span class="n">nsamples_per_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">nsample_ratios</span><span class="p">))</span><span class="o">*</span><span class="n">nhf_samples</span>
<span class="n">target_cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">model_costs</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">nsamples_per_model</span><span class="p">)</span>
<span class="n">means</span><span class="p">,</span> <span class="n">numerical_var</span><span class="p">,</span> <span class="n">true_var</span> <span class="o">=</span> \
    <span class="n">multifidelity</span><span class="o">.</span><span class="n">estimate_variance</span><span class="p">(</span>
        <span class="n">model_ensemble</span><span class="p">,</span> <span class="n">est</span><span class="p">,</span> <span class="n">target_cost</span><span class="p">,</span> <span class="n">ntrials</span><span class="p">,</span> <span class="n">nsample_ratios</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Theoretical ACV variance&quot;</span><span class="p">,</span> <span class="n">true_var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Achieved ACV variance&quot;</span><span class="p">,</span> <span class="n">numerical_var</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Theoretical ACV variance 0.014971109874541333
Achieved ACV variance 0.01617124648499629
</pre></div>
</div>
<p>Let us also plot the distribution of these estimators</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">means</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">ntrials</span><span class="o">//</span><span class="mi">100</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$Q_{0, N}$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">means</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">ntrials</span><span class="o">//</span><span class="mi">100</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$Q_{0, N, </span><span class="si">%d</span><span class="s1">}^\mathrm</span><span class="si">{CV}</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="n">nsample_ratios</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">nsample_ratios</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">100</span><span class="p">])</span>
<span class="n">nsamples_per_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">nsample_ratios</span><span class="p">))</span><span class="o">*</span><span class="n">nhf_samples</span>
<span class="n">target_cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">model_costs</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">nsamples_per_model</span><span class="p">)</span>
<span class="n">means</span><span class="p">,</span> <span class="n">numerical_var</span><span class="p">,</span> <span class="n">true_var</span> <span class="o">=</span> \
    <span class="n">multifidelity</span><span class="o">.</span><span class="n">estimate_variance</span><span class="p">(</span>
        <span class="n">model_ensemble</span><span class="p">,</span> <span class="n">est</span><span class="p">,</span> <span class="n">target_cost</span><span class="p">,</span> <span class="n">ntrials</span><span class="p">,</span> <span class="n">nsample_ratios</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">means</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">ntrials</span><span class="o">//</span><span class="mi">100</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$Q_{0, N, </span><span class="si">%d</span><span class="s1">}^\mathrm</span><span class="si">{CV}</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="n">nsample_ratios</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$E[Q_0]$&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_approximate_control_variate_monte_carlo_002.png" srcset="../../_images/sphx_glr_plot_approximate_control_variate_monte_carlo_002.png" alt="plot approximate control variate monte carlo" class = "sphx-glr-single-img"/><p>For a fixed number of high-fidelity evaluations <span class="math notranslate nohighlight">\(N\)</span> the ACVMC variance reduction will converge to the CVMC variance reduction. Try changing <span class="math notranslate nohighlight">\(N\)</span>.</p>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h2>
<dl class="citation">
<dt class="label" id="ggejjcp2020"><span class="brackets">GGEJJCP2020</span></dt>
<dd><p><a class="reference external" href="https://doi.org/10.1016/j.jcp.2020.109257">A generalized approximate control variate framework for multifidelity uncertainty quantification,  Journal of Computational Physics,  408:109257, 2020.</a></p>
</dd>
</dl>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  0.703 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-tutorials-multi-fidelity-plot-approximate-control-variate-monte-carlo-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/a16d77e9e7aa03c9bf0384cc74f9053b/plot_approximate_control_variate_monte_carlo.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_approximate_control_variate_monte_carlo.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/598eea04db8f7d407fe64bf60495fa72/plot_approximate_control_variate_monte_carlo.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_approximate_control_variate_monte_carlo.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="plot_control_variate_monte_carlo.html" class="btn btn-neutral float-left" title="Control Variate Monte Carlo" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="plot_multi_level_monte_carlo.html" class="btn btn-neutral float-right" title="Multi-level Monte Carlo" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019 National Technology &amp; Engineering Solutions of Sandia, LLC (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the U.S. Government retains certain rights in this software..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>